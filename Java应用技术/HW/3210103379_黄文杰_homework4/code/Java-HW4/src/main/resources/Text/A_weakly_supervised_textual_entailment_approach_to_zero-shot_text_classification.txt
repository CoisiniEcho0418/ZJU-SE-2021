Title: A weakly supervised textual entailment approach to zero-shot text classification
Authors: Marc Pàmies, Joan Llop, Francesco Multari, Nicolau Duran-Silva, César Parra-Rojas, Aitor Gonzalez-Agirre, Francesco Alessandro Massucci, Marta Villegas
Abstract: Zero-shot text classification is a widely studied task that deals with a lack of annotated data. The most common approach is to reformulate it as a textual entailment problem, enabling classification into unseen classes. This work explores an effective approach that trains on a weakly supervised dataset generated from traditional classification data. We empirically study the relation between the performance of the entailment task, which is used as a proxy, and the target zero-shot text classification task. Our findings reveal that there is no linear correlation between both tasks, to the extent that it can be detrimental to lengthen the fine-tuning process even when the model is still learning, and propose a straightforward method to stop training on time. As a proof of concept, we introduce a domain-specific zero-shot text classifier that was trained on Microsoft Academic Graph data. The model, called SCIroShot, achieves state-of-the-art performance in the scientific domain and competitive results in other areas. Both the model and evaluation benchmark are publicly available on HuggingFace and GitHub.
Cite (Informal):A weakly supervised textual entailment approach to zero-shot text classification (Pàmies et al., EACL 2023)
Year:2023
Venue:EACL
Video:https://aclanthology.org/2023.eacl-main.22.mp4
Anthology ID:2023.eacl-main.22
Bibkey:pamies-etal-2023-weakly
Volume:Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics
URL:https://aclanthology.org/2023.eacl-main.22
PDF:https://aclanthology.org/2023.eacl-main.22.pdf
Month:May
SIG:
Address:Dubrovnik, Croatia
Language:
Cite (ACL):Marc Pàmies, Joan Llop, Francesco Multari, Nicolau Duran-Silva, César Parra-Rojas, Aitor Gonzalez-Agirre, Francesco Alessandro Massucci, and Marta Villegas. 2023. A weakly supervised textual entailment approach to zero-shot text classification. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 286–296, Dubrovnik, Croatia. Association for Computational Linguistics.
Editors:Andreas Vlachos,Isabelle Augenstein
Note:
Pages:286–296
DOI:10.18653/v1/2023.eacl-main.22
Publisher:Association for Computational Linguistics
BibTex: @inproceedings{pamies-etal-2023-weakly,
    title = "A weakly supervised textual entailment approach to zero-shot text classification",
    author = "P{\`a}mies, Marc  and
      Llop, Joan  and
      Multari, Francesco  and
      Duran-Silva, Nicolau  and
      Parra-Rojas, C{\'e}sar  and
      Gonzalez-Agirre, Aitor  and
      Massucci, Francesco Alessandro  and
      Villegas, Marta",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.22",
    doi = "10.18653/v1/2023.eacl-main.22",
    pages = "286--296",
    abstract = "Zero-shot text classification is a widely studied task that deals with a lack of annotated data. The most common approach is to reformulate it as a textual entailment problem, enabling classification into unseen classes. This work explores an effective approach that trains on a weakly supervised dataset generated from traditional classification data. We empirically study the relation between the performance of the entailment task, which is used as a proxy, and the target zero-shot text classification task. Our findings reveal that there is no linear correlation between both tasks, to the extent that it can be detrimental to lengthen the fine-tuning process even when the model is still learning, and propose a straightforward method to stop training on time. As a proof of concept, we introduce a domain-specific zero-shot text classifier that was trained on Microsoft Academic Graph data. The model, called SCIroShot, achieves state-of-the-art performance in the scientific domain and competitive results in other areas. Both the model and evaluation benchmark are publicly available on HuggingFace and GitHub.",
}

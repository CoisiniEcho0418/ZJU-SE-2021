Title: Learning Action Conditions from Instructional Manuals for Instruction Understanding
Authors: Te-Lin Wu, Caiqi Zhang, Qingyuan Hu, Alexander Spangher, Nanyun Peng
Abstract: The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action condition inference, which extracts mentions of preconditions and postconditions of actions in instructional manuals. We propose a weakly supervised approach utilizing automatically constructed large-scale training instances from online instructions, and curate a densely human-annotated and validated dataset to study how well the current NLP models do on the proposed task. We design two types of models differ by whether contextualized and global information is leveraged, as well as various combinations of heuristics to construct the weak supervisions.Our experiments show a > 20% F1-score improvement with considering the entire instruction contexts and a > 6% F1-score benefit with the proposed heuristics. However, the best performing model is still well-behind human performance.
Cite (Informal):Learning Action Conditions from Instructional Manuals for Instruction Understanding (Wu et al., ACL 2023)
Year:2023
Venue:ACL
Anthology ID:2023.acl-long.170
Bibkey:wu-etal-2023-learning
Volume:Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
URL:https://aclanthology.org/2023.acl-long.170
PDF:https://aclanthology.org/2023.acl-long.170.pdf
Month:July
SIG:
Address:Toronto, Canada
Language:
Cite (ACL):Te-Lin Wu, Caiqi Zhang, Qingyuan Hu, Alexander Spangher, and Nanyun Peng. 2023. Learning Action Conditions from Instructional Manuals for Instruction Understanding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3023–3043, Toronto, Canada. Association for Computational Linguistics.
Editors:Anna Rogers,Jordan Boyd-Graber,Naoaki Okazaki
Note:
Pages:3023–3043
DOI:10.18653/v1/2023.acl-long.170
Publisher:Association for Computational Linguistics
BibTex: @inproceedings{wu-etal-2023-learning,
    title = "Learning Action Conditions from Instructional Manuals for Instruction Understanding",
    author = "Wu, Te-Lin  and
      Zhang, Caiqi  and
      Hu, Qingyuan  and
      Spangher, Alexander  and
      Peng, Nanyun",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.170",
    doi = "10.18653/v1/2023.acl-long.170",
    pages = "3023--3043",
    abstract = "The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action condition inference, which extracts mentions of preconditions and postconditions of actions in instructional manuals. We propose a weakly supervised approach utilizing automatically constructed large-scale training instances from online instructions, and curate a densely human-annotated and validated dataset to study how well the current NLP models do on the proposed task. We design two types of models differ by whether contextualized and global information is leveraged, as well as various combinations of heuristics to construct the weak supervisions.Our experiments show a {\textgreater} 20{\%} F1-score improvement with considering the entire instruction contexts and a {\textgreater} 6{\%} F1-score benefit with the proposed heuristics. However, the best performing model is still well-behind human performance.",
}

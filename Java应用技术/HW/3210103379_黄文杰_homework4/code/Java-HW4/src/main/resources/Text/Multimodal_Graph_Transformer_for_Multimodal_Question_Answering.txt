Title: Multimodal Graph Transformer for Multimodal Question Answering
Authors: Xuehai He, Xin Wang
Abstract: Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph information significantly improves the inferring ability and helps align features from different modalities. We validate the effectiveness of Multimodal Graph Transformer over its Transformer baselines on GQA, VQAv2, and MultiModalQA datasets.
Cite (Informal):Multimodal Graph Transformer for Multimodal Question Answering (He & Wang, EACL 2023)
Year:2023
Venue:EACL
Video:https://aclanthology.org/2023.eacl-main.15.mp4
Anthology ID:2023.eacl-main.15
Bibkey:he-wang-2023-multimodal
Volume:Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics
URL:https://aclanthology.org/2023.eacl-main.15
PDF:https://aclanthology.org/2023.eacl-main.15.pdf
Month:May
SIG:
Address:Dubrovnik, Croatia
Language:
Cite (ACL):Xuehai He and Xin Wang. 2023. Multimodal Graph Transformer for Multimodal Question Answering. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 189–200, Dubrovnik, Croatia. Association for Computational Linguistics.
Editors:Andreas Vlachos,Isabelle Augenstein
Note:
Pages:189–200
DOI:10.18653/v1/2023.eacl-main.15
Publisher:Association for Computational Linguistics
BibTex: @inproceedings{he-wang-2023-multimodal,
    title = "Multimodal Graph Transformer for Multimodal Question Answering",
    author = "He, Xuehai  and
      Wang, Xin",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.15",
    doi = "10.18653/v1/2023.eacl-main.15",
    pages = "189--200",
    abstract = "Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph information significantly improves the inferring ability and helps align features from different modalities. We validate the effectiveness of Multimodal Graph Transformer over its Transformer baselines on GQA, VQAv2, and MultiModalQA datasets.",
}

Title: ON-TRAC Consortium Systems for the IWSLT 2023 Dialectal and Low-resource Speech Translation Tasks
Authors: Antoine Laurent, Souhir Gahbiche, Ha Nguyen, Haroun Elleuch, Fethi Bougares, Antoine Thiol, Hugo Riguidel, Salima Mdhaffar, Gaëlle Laperrière, Lucas Maison, Sameer Khurana, Yannick Estève
Abstract: This paper describes the ON-TRAC consortium speech translation systems developed for IWSLT 2023 evaluation campaign. Overall, we participated in three speech translation tracks featured in the low-resource and dialect speech translation shared tasks, namely; i) spoken Tamasheq to written French, ii) spoken Pashto to written French, and iii) spoken Tunisian to written English. All our primary submissions are based on the end-to-end speech-to-text neural architecture using a pretrained SAMU-XLSR model as a speech encoder and a mbart model as a decoder. The SAMU-XLSR model is built from the XLS-R 128 in order to generate language agnostic sentence-level embeddings. This building is driven by the LaBSE model trained on multilingual text dataset. This architecture allows us to improve the input speech representations and achieve significant improvements compared to conventional end-to-end speech translation systems.
Cite (Informal):ON-TRAC Consortium Systems for the IWSLT 2023 Dialectal and Low-resource Speech Translation Tasks (Laurent et al., IWSLT 2023)
Year:2023
Venue:IWSLT
Anthology ID:2023.iwslt-1.18
Bibkey:laurent-etal-2023-trac
Volume:Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)
URL:https://aclanthology.org/2023.iwslt-1.18
PDF:https://aclanthology.org/2023.iwslt-1.18.pdf
Month:July
SIG:SIGSLT
Address:Toronto, Canada (in-person and online)
Language:
Cite (ACL):Antoine Laurent, Souhir Gahbiche, Ha Nguyen, Haroun Elleuch, Fethi Bougares, Antoine Thiol, Hugo Riguidel, Salima Mdhaffar, Gaëlle Laperrière, Lucas Maison, Sameer Khurana, and Yannick Estève. 2023. ON-TRAC Consortium Systems for the IWSLT 2023 Dialectal and Low-resource Speech Translation Tasks. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 219–226, Toronto, Canada (in-person and online). Association for Computational Linguistics.
Editors:Elizabeth Salesky,Marcello Federico,Marine Carpuat
Note:
Pages:219–226
DOI:10.18653/v1/2023.iwslt-1.18
Publisher:Association for Computational Linguistics
BibTex: @inproceedings{laurent-etal-2023-trac,
    title = "{ON}-{TRAC} Consortium Systems for the {IWSLT} 2023 Dialectal and Low-resource Speech Translation Tasks",
    author = {Laurent, Antoine  and
      Gahbiche, Souhir  and
      Nguyen, Ha  and
      Elleuch, Haroun  and
      Bougares, Fethi  and
      Thiol, Antoine  and
      Riguidel, Hugo  and
      Mdhaffar, Salima  and
      Laperri{\`e}re, Ga{\"e}lle  and
      Maison, Lucas  and
      Khurana, Sameer  and
      Est{\`e}ve, Yannick},
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.18",
    doi = "10.18653/v1/2023.iwslt-1.18",
    pages = "219--226",
    abstract = "This paper describes the ON-TRAC consortium speech translation systems developed for IWSLT 2023 evaluation campaign. Overall, we participated in three speech translation tracks featured in the low-resource and dialect speech translation shared tasks, namely; i) spoken Tamasheq to written French, ii) spoken Pashto to written French, and iii) spoken Tunisian to written English. All our primary submissions are based on the end-to-end speech-to-text neural architecture using a pretrained SAMU-XLSR model as a speech encoder and a mbart model as a decoder. The SAMU-XLSR model is built from the XLS-R 128 in order to generate language agnostic sentence-level embeddings. This building is driven by the LaBSE model trained on multilingual text dataset. This architecture allows us to improve the input speech representations and achieve significant improvements compared to conventional end-to-end speech translation systems.",
}

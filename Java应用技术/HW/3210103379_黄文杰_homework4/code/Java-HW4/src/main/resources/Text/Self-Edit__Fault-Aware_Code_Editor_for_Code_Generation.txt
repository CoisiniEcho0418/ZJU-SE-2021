Title: Self-Edit: Fault-Aware Code Editor for Code Generation
Authors: Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin
Abstract: Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.
Cite (Informal):Self-Edit: Fault-Aware Code Editor for Code Generation (Zhang et al., ACL 2023)
Year:2023
Venue:ACL
Anthology ID:2023.acl-long.45
Version 2:2023.acl-long.45v2
Bibkey:zhang-etal-2023-self
Volume:Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
URL:https://aclanthology.org/2023.acl-long.45
PDF:https://aclanthology.org/2023.acl-long.45.pdf
Month:July
SIG:
Address:Toronto, Canada
Language:
Cite (ACL):Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023. Self-Edit: Fault-Aware Code Editor for Code Generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 769–787, Toronto, Canada. Association for Computational Linguistics.
Version 3:2023.acl-long.45v3
Editors:Anna Rogers,Jordan Boyd-Graber,Naoaki Okazaki
Note:
Original:2023.acl-long.45v1
Pages:769–787
DOI:10.18653/v1/2023.acl-long.45
Publisher:Association for Computational Linguistics
BibTex: @inproceedings{zhang-etal-2023-self,
    title = "Self-Edit: Fault-Aware Code Editor for Code Generation",
    author = "Zhang, Kechi  and
      Li, Zhuo  and
      Li, Jia  and
      Li, Ge  and
      Jin, Zhi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.45",
    doi = "10.18653/v1/2023.acl-long.45",
    pages = "769--787",
    abstract = "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89{\%} on APPS-dev, 31{\%} on APPS-test, and 48{\%} on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
}

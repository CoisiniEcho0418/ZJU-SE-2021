Title: Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation
Authors: Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li
Abstract: It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student’s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher’s learning process. By prioritizing samples that are likely to enhance the student’s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
Cite (Informal):Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation (Ren et al., ACL 2023)
Year:2023
Venue:ACL
Anthology ID:2023.acl-long.111
Bibkey:ren-etal-2023-tailoring
Volume:Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
URL:https://aclanthology.org/2023.acl-long.111
PDF:https://aclanthology.org/2023.acl-long.111.pdf
Month:July
SIG:
Address:Toronto, Canada
Language:
Cite (ACL):Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, and Mu Li. 2023. Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1990–2006, Toronto, Canada. Association for Computational Linguistics.
Editors:Anna Rogers,Jordan Boyd-Graber,Naoaki Okazaki
Note:
Pages:1990–2006
DOI:10.18653/v1/2023.acl-long.111
Publisher:Association for Computational Linguistics
BibTex: @inproceedings{ren-etal-2023-tailoring,
    title = "Tailoring Instructions to Student{'}s Learning Levels Boosts Knowledge Distillation",
    author = "Ren, Yuxin  and
      Zhong, Zihan  and
      Shi, Xingjian  and
      Zhu, Yi  and
      Yuan, Chun  and
      Li, Mu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.111",
    doi = "10.18653/v1/2023.acl-long.111",
    pages = "1990--2006",
    abstract = "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student{'}s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher{'}s learning process. By prioritizing samples that are likely to enhance the student{'}s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.",
}
